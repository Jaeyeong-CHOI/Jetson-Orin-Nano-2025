{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62034352",
   "metadata": {},
   "source": [
    "\n",
    "# LCEL ì™„ì „ ê¸°ì´ˆ ì‹¤ìŠµ ë…¸íŠ¸ë¶ (LangChain 0.3.x)  \n",
    "**Runnable/LCEL** ê¸°ì´ˆë¶€í„° NVIDIA API ì—°ë™ê¹Œì§€ í•œ ë²ˆì— ìµíˆëŠ” ì£¼í”¼í„° ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤.  \n",
    "ëª©í‘œ:\n",
    "1) LCEL í•µì‹¬ ì»¨ì…‰ ì´í•´: Runnable, `|` íŒŒì´í”„, `invoke/batch/stream`  \n",
    "2) ì…ë ¥/ì¶œë ¥ ê°€ê³µ, ë³‘ë ¬ ì‹¤í–‰, í”„ë¡¬í”„íŠ¸/íŒŒì„œ ì‚¬ìš©ë²•  \n",
    "3) **NVIDIA API ëª¨ë¸**ì„ LCEL ë…¸ë“œë¡œ ê°ì‹¸ì„œ ìŠ¤íŠ¸ë¦¬ë°/ë‹¨ë°œ í˜¸ì¶œí•˜ê¸°  \n",
    "4) ê°„ë‹¨ ì²´ì¸: ì…ë ¥ â†’ í”„ë¡¬í”„íŠ¸ â†’ NVIDIA LLM â†’ íŒŒì„œ\n",
    "\n",
    "> ë³¸ ë…¸íŠ¸ë¶ì€ **LangChain 0.3.x** ë²„ì „ì— ë§ëŠ” ì„í¬íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666dbf13",
   "metadata": {},
   "source": [
    "\n",
    "## 0. í™˜ê²½ ì ê²€ & í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "ì£¼í”¼í„° **í˜„ì¬ ì»¤ë„**ì— ë°”ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤. (ì»¤ë„/ì¸í„°í”„ë¦¬í„° ë¶ˆì¼ì¹˜ ì´ìŠˆ ë°©ì§€)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9dd593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\jaeye\\anaconda3\\python.exe\n",
      "langchain: 0.3.27\n",
      "langchain_core: 0.3.74\n",
      "langchain_core ìœ„ì¹˜: C:\\Users\\jaeye\\anaconda3\\Lib\\site-packages\\langchain_core\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *pkgs])\n",
    "\n",
    "base_pkgs = [\n",
    "    \"langchain-core>=0.3.72\",\n",
    "    \"langchain>=0.3.27\",\n",
    "]\n",
    "ensure(base_pkgs)\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "import langchain, langchain_core\n",
    "print(\"langchain:\", langchain.__version__)\n",
    "print(\"langchain_core:\", langchain_core.__version__)\n",
    "print(\"langchain_core ìœ„ì¹˜:\", langchain_core.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf84606",
   "metadata": {},
   "source": [
    "\n",
    "## 1. LCEL ìµœì†Œ ë‹¨ìœ„: Runnable + íŒŒì´í”„ + `invoke()`\n",
    "- **RunnableLambda**: íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ Runnableë¡œ ê°ì‹¸ì„œ LCEL íŒŒì´í”„ë¼ì¸ì— ì—°ê²° ê°€ëŠ¥  \n",
    "- `|` ì—°ì‚°ì: **ì²´ì¸ ì—°ê²°**(ì• ê²°ê³¼ â†’ ë’¤ ì…ë ¥)  \n",
    "- `invoke(x)`: ë‹¨ì¼ ì…ë ¥ ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f91654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_basic.invoke(10) = 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x): \n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x): \n",
    "    return x * 2\n",
    "\n",
    "add = RunnableLambda(add_five)\n",
    "mul2 = RunnableLambda(multiply_by_two)\n",
    "\n",
    "chain_basic = add | mul2  # (x + 5) -> * 2\n",
    "\n",
    "print(\"chain_basic.invoke(10) =\", chain_basic.invoke(10))  # 30 ê¸°ëŒ€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d4b38",
   "metadata": {},
   "source": [
    "\n",
    "## 2. ì…ë ¥/ì¶œë ¥ ê°€ê³µ: `RunnableMap`, `RunnablePassthrough`\n",
    "- **RunnableMap**: ì…ë ¥ í•˜ë‚˜ë¡œ **ì—¬ëŸ¬ í‚¤**ë¥¼ ìƒì„± (ì „ì²˜ë¦¬/íŠ¹ì§•ì¶”ì¶œì— ìœ ìš©)  \n",
    "- **RunnablePassthrough**: ì›ë³¸ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ (ìŠ¤í‚µ ì—°ê²°ìš©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b647fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_map.invoke(10) = 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "prepare = RunnableMap({\n",
    "    \"orig\": RunnablePassthrough(),\n",
    "    \"plus5\": RunnableLambda(lambda x: x + 5),\n",
    "})\n",
    "\n",
    "use_plus5 = RunnableLambda(lambda d: d[\"plus5\"] * 2)\n",
    "#use_plus5 = RunnableLambda(lambda d: d[\"orig\"] * 2)\n",
    "\n",
    "chain_map = prepare | use_plus5\n",
    "print(\"chain_map.invoke(10) =\", chain_map.invoke(10))  # 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e9adf",
   "metadata": {},
   "source": [
    "\n",
    "## 3. ë³‘ë ¬ ì‹¤í–‰: `RunnableParallel`\n",
    "ë™ì¼ ì…ë ¥ì„ **ì—¬ëŸ¬ ê°ˆë˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬** í›„ dictë¡œ ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec7e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel.invoke(3) = {'square': 9, 'cube': 27, 'plus5': 8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"square\": RunnableLambda(lambda x: x * x),\n",
    "    \"cube\":   RunnableLambda(lambda x: x * x * x),\n",
    "    \"plus5\":  RunnableLambda(lambda x: x + 5),\n",
    "})\n",
    "\n",
    "print(\"parallel.invoke(3) =\", parallel.invoke(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f1f07",
   "metadata": {},
   "source": [
    "\n",
    "## 4. ë°°ì¹˜/ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰: `.batch()`, `.stream()`\n",
    "- **batch**: ë¦¬ìŠ¤íŠ¸ ì…ë ¥ ì¼ê´„ ì²˜ë¦¬ â†’ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸  \n",
    "- **stream**: ì œë„ˆë ˆì´í„° í† í° ìŠ¤íŠ¸ë¦¬ë° (LLM í† í°/í”„ë¡œê·¸ë ˆìŠ¤ ë“±)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065cbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_basic.batch([1,2,3,10]) = [12, 14, 16, 30]\n",
      "stream letters: H E L L O "
     ]
    }
   ],
   "source": [
    "\n",
    "# 4-1. batch\n",
    "print(\"chain_basic.batch([1,2,3,10]) =\", chain_basic.batch([1, 2, 3, 10]))\n",
    "\n",
    "# 4-2. stream: ê°„ë‹¨í•œ ê¸€ì ìŠ¤íŠ¸ë¦¬ë¨¸ ì˜ˆì‹œ\n",
    "def stream_letters(text):\n",
    "    for ch in text:\n",
    "        yield ch\n",
    "\n",
    "stream_node = RunnableLambda(stream_letters)\n",
    "\n",
    "print(\"stream letters:\", end=\" \")\n",
    "for t in stream_node.stream(\"HELLO\"):\n",
    "    print(t, end=\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518f670",
   "metadata": {},
   "source": [
    "\n",
    "## 5. í”„ë¡¬í”„íŠ¸ + ì¶œë ¥ íŒŒì„œ: `ChatPromptTemplate`, `StrOutputParser`\n",
    "- **ChatPromptTemplate**: ì‹œìŠ¤í…œ/ìœ ì € ë“± ì—­í•  ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±  \n",
    "- **StrOutputParser**: ë¬¸ìì—´ë¡œ ê²°ê³¼ íŒŒì‹± (ê°„ë‹¨ ì¼€ì´ìŠ¤)\n",
    "> ì•„ì§ LLMì€ ì—°ê²°í•˜ì§€ ì•Šê³ , í¬ë§·íŒ…ê¹Œì§€ë§Œ í™•ì¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30518ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í¬ë§· ê²°ê³¼ ë©”ì‹œì§€ë“¤: [SystemMessage(content='ë„ˆëŠ” í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì•¼.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë‹¤ìŒ ë¬¸ì¥ì„ ìš”ì•½í•´ì¤˜:\\n\\nLangChainì€ íŒŒì´í”„ë¼ì´ë‹ì´ ì‰¬ìš´ LCELì„ ì œê³µí•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì•¼.\"),\n",
    "    (\"human\",  \"ë‹¤ìŒ ë¬¸ì¥ì„ ìš”ì•½í•´ì¤˜:\\n\\n{input}\")\n",
    "])\n",
    "\n",
    "formatted = prompt.invoke({\"input\": \"LangChainì€ íŒŒì´í”„ë¼ì´ë‹ì´ ì‰¬ìš´ LCELì„ ì œê³µí•©ë‹ˆë‹¤.\"})\n",
    "print(\"í¬ë§· ê²°ê³¼ ë©”ì‹œì§€ë“¤:\", formatted.to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcd61a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. NVIDIA APIë¥¼ LCEL ë…¸ë“œë¡œ ê°ì‹¸ê¸° (ìŠ¤íŠ¸ë¦¬ë° í¬í•¨)\n",
    "- ì£¼ì–´ì§„ **NVIDIA Chat Completions** ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©  \n",
    "- **SSE(Stream)**ë¥¼ íŒŒì‹±í•˜ì—¬ í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥  \n",
    "- LCELê³¼ ì—°ê²°í•˜ê¸° ìœ„í•´ **RunnableLambda**ë¡œ ê°ìŒ‰ë‹ˆë‹¤.\n",
    "\n",
    "> **API í‚¤ í•„ìš”**: `nvapi-`ë¡œ ì‹œì‘í•˜ëŠ” **NVIDIA_API_KEY**ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ë„£ê±°ë‚˜, ë…¸íŠ¸ë¶ì—ì„œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    ">\n",
    "> NVIDIA APIëŠ” OpenAI í˜¸í™˜ Chat Completions ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì½”ë“œëŠ” ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í† í°ì„ ë°›ì•„ ì¶œë ¥í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "ëª¨ë¸: mistralai/mixtral-8x7b-instruct-v0.1\n",
    "API í‚¤ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [https://build.nvidia.com/]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8958991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA ë…¸ë“œ ë‹¨ë°œ í˜¸ì¶œ í…ŒìŠ¤íŠ¸:\n",
      "ì•ˆë…•í•˜ì„¸ìš”!  Korean greeting.  How can I help you today?\n",
      "NVIDIA ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸:\n",
      "ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ëª¨ë¸ì€ ê¸°ë¶„ì´ VERY GOOD ìƒíƒœì…ë‹ˆë‹¤. ë§¤ì¼ ìƒˆë¡œìš´ ë°ì´í„°ì™€ ê²½í—˜ì„ í†µí•´ ë” ë›°ì–´ë‚œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–¸ì œë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´  warmly welcome  your inquiries. ê°ì‚¬í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, requests\n",
    "from getpass import getpass\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "#os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "\n",
    "# ğŸ”‘ í‚¤ ì„¤ì •\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass(\"NVIDIA_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš” (nvapi-...): \")\n",
    "\n",
    "INVOKE_URL = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ['NVIDIA_API_KEY']}\",\n",
    "}\n",
    "\n",
    "def _extract_stream_token(entry: bytes) -> str:\n",
    "    if not entry:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if entry.startswith(b\"data: \"):\n",
    "            payload = json.loads(entry[6:].decode(\"utf-8\"))\n",
    "            return payload.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def nv_chat_stream(messages,\n",
    "                   model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "                   temperature=0.5,\n",
    "                   top_p=1,\n",
    "                   max_tokens=256):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    with requests.post(INVOKE_URL, headers=HEADERS, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        for line in r.iter_lines():\n",
    "            tok = _extract_stream_token(line)\n",
    "            if tok:\n",
    "                yield tok\n",
    "\n",
    "def nv_chat_once(messages,\n",
    "                 model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "                 temperature=0.5,\n",
    "                 top_p=1,\n",
    "                 max_tokens=256) -> str:\n",
    "    return \"\".join(nv_chat_stream(messages, model, temperature, top_p, max_tokens))\n",
    "\n",
    "def _nv_node_fn(user_input: str) -> str:\n",
    "    msgs = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    return nv_chat_once(msgs)\n",
    "\n",
    "nv_node = RunnableLambda(_nv_node_fn)\n",
    "\n",
    "print(\"NVIDIA ë…¸ë“œ ë‹¨ë°œ í˜¸ì¶œ í…ŒìŠ¤íŠ¸:\")\n",
    "print(nv_node.invoke(\"ì•ˆë…•? í•œêµ­ì–´ë¡œ ì§§ê²Œ ì¸ì‚¬í•´ì¤˜.\"))\n",
    "\n",
    "print(\"NVIDIA ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸:\")\n",
    "for tok in nv_node.stream(\"ë„ˆëŠ” í•œêµ­ì–´ë¥¼ ì“°ëŠ” ëª¨ë¸ì´ì•¼. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ?\"):\n",
    "    print(tok, end=\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9450eb",
   "metadata": {},
   "source": [
    "\n",
    "## 7. ì¡°í•©: ì…ë ¥ â†’ í”„ë¡¬í”„íŠ¸ â†’ NVIDIA LLM â†’ ë¬¸ìì—´ íŒŒì„œ (LCEL ì²´ì¸)\n",
    "- í”„ë¡¬í”„íŠ¸ ì¶œë ¥(Messages)ì„ **ë‹¨ì¼ user ë©”ì‹œì§€**ë¡œ í•©ì³ NVIDIA í¬ë§·ì— ë§ì¶¥ë‹ˆë‹¤.  \n",
    "- LCEL íŒŒì´í”„: `RunnablePassthrough`ë¡œ ì…ë ¥ì„ dictí™” â†’ `ChatPromptTemplate` â†’ ë³€í™˜ â†’ NVIDIA í˜¸ì¶œ â†’ `StrOutputParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb8707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Donald TrumpëŠ” 2017ë…„ 1ì›” 1ì¼ ~ 2021ë…„ 1ì›” 20ì¼ê¹Œì§€ ë¯¸êµ­ ëŒ€í†µë ¹ì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "- íŠ¸ëŸ¼í”„ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤  Tycoonìœ¼ë¡œì„œ ìœ ëª…í–ˆìœ¼ë©°, ë¯¸êµ­ ì •ì¹˜ì— ì²˜ìŒ ì…ë¬¸í•œ ê²ƒì€ 2015ë…„ì…ë‹ˆë‹¤.\n",
      "- ê·¸ëŠ” ë¶í•œ í•µ ìœ„í˜‘, ì´ë¯¼ ë¬¸ì œ ë“± ë‹¤ì–‘í•œ êµ­ì œì , êµ­ë‚´ì  ë¬¸ì œì— ëŒ€ì‘í•˜ë©° í™œì•½í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summ_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì „ë¬¸ ìš”ì•½ê°€ì•¼. í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ, 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•œë‹¤.\"),\n",
    "    (\"human\",  \"ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\\n\\n{input}\")\n",
    "])\n",
    "\n",
    "def to_nvidia_messages(template_out) -> list:\n",
    "    msgs = template_out.to_messages()\n",
    "    text = \"\\n\".join(m.content for m in msgs)\n",
    "    return [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "def call_nvidia_with_messages(messages: list) -> str:\n",
    "    return nv_chat_once(messages)\n",
    "\n",
    "nv_messages_node = RunnableLambda(to_nvidia_messages)\n",
    "nv_call_node = RunnableLambda(call_nvidia_with_messages)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "ragless_chain = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | summ_prompt\n",
    "    | nv_messages_node\n",
    "    | nv_call_node\n",
    "    | to_str\n",
    ")\n",
    "\n",
    "print(ragless_chain.invoke(\"ë¯¸êµ­ì˜ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7afbfa",
   "metadata": {},
   "source": [
    "\n",
    "## ë§ˆë¬´ë¦¬\n",
    "- ì—¬ê¸°ê¹Œì§€ë¡œ **LCEL í•µì‹¬ íë¦„**ê³¼ **NVIDIA API ì—°ë™**ì„ ìµí˜”ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
