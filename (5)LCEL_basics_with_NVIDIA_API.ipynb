{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62034352",
   "metadata": {},
   "source": [
    "\n",
    "# LCEL 완전 기초 실습 노트북 (LangChain 0.3.x)  \n",
    "**Runnable/LCEL** 기초부터 NVIDIA API 연동까지 한 번에 익히는 주피터 노트북입니다.  \n",
    "목표:\n",
    "1) LCEL 핵심 컨셉 이해: Runnable, `|` 파이프, `invoke/batch/stream`  \n",
    "2) 입력/출력 가공, 병렬 실행, 프롬프트/파서 사용법  \n",
    "3) **NVIDIA API 모델**을 LCEL 노드로 감싸서 스트리밍/단발 호출하기  \n",
    "4) 간단 체인: 입력 → 프롬프트 → NVIDIA LLM → 파서\n",
    "\n",
    "> 본 노트북은 **LangChain 0.3.x** 버전에 맞는 임포트 경로를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666dbf13",
   "metadata": {},
   "source": [
    "\n",
    "## 0. 환경 점검 & 필요한 패키지 설치\n",
    "주피터 **현재 커널**에 바로 설치합니다. (커널/인터프리터 불일치 이슈 방지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9dd593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\jaeye\\anaconda3\\python.exe\n",
      "langchain: 0.3.27\n",
      "langchain_core: 0.3.74\n",
      "langchain_core 위치: C:\\Users\\jaeye\\anaconda3\\Lib\\site-packages\\langchain_core\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *pkgs])\n",
    "\n",
    "base_pkgs = [\n",
    "    \"langchain-core>=0.3.72\",\n",
    "    \"langchain>=0.3.27\",\n",
    "]\n",
    "ensure(base_pkgs)\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "import langchain, langchain_core\n",
    "print(\"langchain:\", langchain.__version__)\n",
    "print(\"langchain_core:\", langchain_core.__version__)\n",
    "print(\"langchain_core 위치:\", langchain_core.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf84606",
   "metadata": {},
   "source": [
    "\n",
    "## 1. LCEL 최소 단위: Runnable + 파이프 + `invoke()`\n",
    "- **RunnableLambda**: 파이썬 함수를 Runnable로 감싸서 LCEL 파이프라인에 연결 가능  \n",
    "- `|` 연산자: **체인 연결**(앞 결과 → 뒤 입력)  \n",
    "- `invoke(x)`: 단일 입력 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f91654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_basic.invoke(10) = 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x): \n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x): \n",
    "    return x * 2\n",
    "\n",
    "add = RunnableLambda(add_five)\n",
    "mul2 = RunnableLambda(multiply_by_two)\n",
    "\n",
    "chain_basic = add | mul2  # (x + 5) -> * 2\n",
    "\n",
    "print(\"chain_basic.invoke(10) =\", chain_basic.invoke(10))  # 30 기대\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d4b38",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 입력/출력 가공: `RunnableMap`, `RunnablePassthrough`\n",
    "- **RunnableMap**: 입력 하나로 **여러 키**를 생성 (전처리/특징추출에 유용)  \n",
    "- **RunnablePassthrough**: 원본 입력을 그대로 전달 (스킵 연결용)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b647fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_map.invoke(10) = 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "prepare = RunnableMap({\n",
    "    \"orig\": RunnablePassthrough(),\n",
    "    \"plus5\": RunnableLambda(lambda x: x + 5),\n",
    "})\n",
    "\n",
    "use_plus5 = RunnableLambda(lambda d: d[\"plus5\"] * 2)\n",
    "#use_plus5 = RunnableLambda(lambda d: d[\"orig\"] * 2)\n",
    "\n",
    "chain_map = prepare | use_plus5\n",
    "print(\"chain_map.invoke(10) =\", chain_map.invoke(10))  # 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e9adf",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 병렬 실행: `RunnableParallel`\n",
    "동일 입력을 **여러 갈래로 병렬 처리** 후 dict로 결과를 받습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec7e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel.invoke(3) = {'square': 9, 'cube': 27, 'plus5': 8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"square\": RunnableLambda(lambda x: x * x),\n",
    "    \"cube\":   RunnableLambda(lambda x: x * x * x),\n",
    "    \"plus5\":  RunnableLambda(lambda x: x + 5),\n",
    "})\n",
    "\n",
    "print(\"parallel.invoke(3) =\", parallel.invoke(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f1f07",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 배치/스트림 실행: `.batch()`, `.stream()`\n",
    "- **batch**: 리스트 입력 일괄 처리 → 결과 리스트  \n",
    "- **stream**: 제너레이터 토큰 스트리밍 (LLM 토큰/프로그레스 등)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065cbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_basic.batch([1,2,3,10]) = [12, 14, 16, 30]\n",
      "stream letters: H E L L O "
     ]
    }
   ],
   "source": [
    "\n",
    "# 4-1. batch\n",
    "print(\"chain_basic.batch([1,2,3,10]) =\", chain_basic.batch([1, 2, 3, 10]))\n",
    "\n",
    "# 4-2. stream: 간단한 글자 스트리머 예시\n",
    "def stream_letters(text):\n",
    "    for ch in text:\n",
    "        yield ch\n",
    "\n",
    "stream_node = RunnableLambda(stream_letters)\n",
    "\n",
    "print(\"stream letters:\", end=\" \")\n",
    "for t in stream_node.stream(\"HELLO\"):\n",
    "    print(t, end=\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518f670",
   "metadata": {},
   "source": [
    "\n",
    "## 5. 프롬프트 + 출력 파서: `ChatPromptTemplate`, `StrOutputParser`\n",
    "- **ChatPromptTemplate**: 시스템/유저 등 역할 기반 프롬프트 구성  \n",
    "- **StrOutputParser**: 문자열로 결과 파싱 (간단 케이스)\n",
    "> 아직 LLM은 연결하지 않고, 포맷팅까지만 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30518ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "포맷 결과 메시지들: [SystemMessage(content='너는 한국어로 간결하게 답하는 도우미야.', additional_kwargs={}, response_metadata={}), HumanMessage(content='다음 문장을 요약해줘:\\n\\nLangChain은 파이프라이닝이 쉬운 LCEL을 제공합니다.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 한국어로 간결하게 답하는 도우미야.\"),\n",
    "    (\"human\",  \"다음 문장을 요약해줘:\\n\\n{input}\")\n",
    "])\n",
    "\n",
    "formatted = prompt.invoke({\"input\": \"LangChain은 파이프라이닝이 쉬운 LCEL을 제공합니다.\"})\n",
    "print(\"포맷 결과 메시지들:\", formatted.to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcd61a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. NVIDIA API를 LCEL 노드로 감싸기 (스트리밍 포함)\n",
    "- 주어진 **NVIDIA Chat Completions** 엔드포인트 사용  \n",
    "- **SSE(Stream)**를 파싱하여 토큰 단위로 출력  \n",
    "- LCEL과 연결하기 위해 **RunnableLambda**로 감쌉니다.\n",
    "\n",
    "> **API 키 필요**: `nvapi-`로 시작하는 **NVIDIA_API_KEY**를 환경변수로 넣거나, 노트북에서 입력받습니다.\n",
    ">\n",
    "> NVIDIA API는 OpenAI 호환 Chat Completions 엔드포인트를 제공합니다.\n",
    "아래 코드는 스트리밍으로 토큰을 받아 출력하는 예시입니다.\n",
    "모델: mistralai/mixtral-8x7b-instruct-v0.1\n",
    "API 키는 다음 링크에서 받을 수 있습니다. [https://build.nvidia.com/]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8958991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA 노드 단발 호출 테스트:\n",
      "안녕하세요!  Korean greeting.  How can I help you today?\n",
      "NVIDIA 스트리밍 테스트:\n",
      "안녕하세요! 현재 모델은 기분이 VERY GOOD 상태입니다. 매일 새로운 데이터와 경험을 통해 더 뛰어난 서비스를 제공하기 위해 노력하고 있습니다. 언제나 질문이 있으시면  warmly welcome  your inquiries. 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, requests\n",
    "from getpass import getpass\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "#os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "\n",
    "# 🔑 키 설정\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass(\"NVIDIA_API_KEY를 입력하세요 (nvapi-...): \")\n",
    "\n",
    "INVOKE_URL = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ['NVIDIA_API_KEY']}\",\n",
    "}\n",
    "\n",
    "def _extract_stream_token(entry: bytes) -> str:\n",
    "    if not entry:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if entry.startswith(b\"data: \"):\n",
    "            payload = json.loads(entry[6:].decode(\"utf-8\"))\n",
    "            return payload.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def nv_chat_stream(messages,\n",
    "                   model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "                   temperature=0.5,\n",
    "                   top_p=1,\n",
    "                   max_tokens=256):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    with requests.post(INVOKE_URL, headers=HEADERS, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        for line in r.iter_lines():\n",
    "            tok = _extract_stream_token(line)\n",
    "            if tok:\n",
    "                yield tok\n",
    "\n",
    "def nv_chat_once(messages,\n",
    "                 model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "                 temperature=0.5,\n",
    "                 top_p=1,\n",
    "                 max_tokens=256) -> str:\n",
    "    return \"\".join(nv_chat_stream(messages, model, temperature, top_p, max_tokens))\n",
    "\n",
    "def _nv_node_fn(user_input: str) -> str:\n",
    "    msgs = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    return nv_chat_once(msgs)\n",
    "\n",
    "nv_node = RunnableLambda(_nv_node_fn)\n",
    "\n",
    "print(\"NVIDIA 노드 단발 호출 테스트:\")\n",
    "print(nv_node.invoke(\"안녕? 한국어로 짧게 인사해줘.\"))\n",
    "\n",
    "print(\"NVIDIA 스트리밍 테스트:\")\n",
    "for tok in nv_node.stream(\"너는 한국어를 쓰는 모델이야. 오늘 기분이 어때?\"):\n",
    "    print(tok, end=\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9450eb",
   "metadata": {},
   "source": [
    "\n",
    "## 7. 조합: 입력 → 프롬프트 → NVIDIA LLM → 문자열 파서 (LCEL 체인)\n",
    "- 프롬프트 출력(Messages)을 **단일 user 메시지**로 합쳐 NVIDIA 포맷에 맞춥니다.  \n",
    "- LCEL 파이프: `RunnablePassthrough`로 입력을 dict화 → `ChatPromptTemplate` → 변환 → NVIDIA 호출 → `StrOutputParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb8707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Donald Trump는 2017년 1월 1일 ~ 2021년 1월 20일까지 미국 대통령이었습니다.\n",
      "- 트럼프는 비즈니스  Tycoon으로서 유명했으며, 미국 정치에 처음 입문한 것은 2015년입니다.\n",
      "- 그는 북한 핵 위협, 이민 문제 등 다양한 국제적, 국내적 문제에 대응하며 활약했습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summ_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 전문 요약가야. 한국어로 핵심만, 3줄 이내로 요약한다.\"),\n",
    "    (\"human\",  \"다음 내용을 요약해줘:\\n\\n{input}\")\n",
    "])\n",
    "\n",
    "def to_nvidia_messages(template_out) -> list:\n",
    "    msgs = template_out.to_messages()\n",
    "    text = \"\\n\".join(m.content for m in msgs)\n",
    "    return [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "def call_nvidia_with_messages(messages: list) -> str:\n",
    "    return nv_chat_once(messages)\n",
    "\n",
    "nv_messages_node = RunnableLambda(to_nvidia_messages)\n",
    "nv_call_node = RunnableLambda(call_nvidia_with_messages)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "ragless_chain = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | summ_prompt\n",
    "    | nv_messages_node\n",
    "    | nv_call_node\n",
    "    | to_str\n",
    ")\n",
    "\n",
    "print(ragless_chain.invoke(\"미국의 트럼프 대통령에 대해 소개해줘\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7afbfa",
   "metadata": {},
   "source": [
    "\n",
    "## 마무리\n",
    "- 여기까지로 **LCEL 핵심 흐름**과 **NVIDIA API 연동**을 익혔습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
